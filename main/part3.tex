\part{Des données à valoriser}

\clearpage
\thispagestyle{empty}
\cleardoublepage

\chapter{Données graphiques, données chiffrées}

\section{Le lien entre le texte et les images du texte}

Dans chaque élément \texttt{<facsimile>}, une balise \texttt{<surface>} définit le bloc de segmentation supérieur (la page). L'élément suivant, \texttt{<graphic>}, indique dans son attribut \texttt{@url} la localisation de l'image de la page segmentée. L'ensemble des images étant stocké dans l'espace alloué au programme \timeus{} au sein du service \sharedocs{} de la TGIR Huma-Num, la localisation consiste en un chemin relatif (\fig{} \ref{fig:facsimile} p. \pageref{fig:facsimile}).

Or ces images sont déjà hébergées par \ia. Elles ont été téléchargées sur le \sharedocs{} dans le but de permettre leur segmentation par \textit{FineReader} et \transkribus{} puis leur \ocr{} par \lse. Leur conservation après l'obtention des fichiers XML n'est plus aussi pertinent que celle des prises de vues des registres prud'homaux effectuées dans des dépôts d'archives, qui n'existent sous aucune autre forme. Rappelons également qu'\ia{} se donne pour objectif d’être un centre stable et durable d’archives digitales ; il est donc peu probable que les images des \odm{} disparaissent de ses serveurs. Il nous a donc été demandé de substituer le chemin local par l'adresse de l'image sur \ia{} (\ann{} \ref{ann:feuille_route}, \issue{} 2) ; cette mission a donné lieu à la publication d'un billet sur le carnet de recherche du programme \timeus\footcite{genero}.

Nous avons apporté deux solutions : l'une employait les URLs basiques des images, l'autre mobilisait les manifestes \iiif{} des volumes. Toutes deux ont donné lieu à des scripts Python dont les fonctionnements sont similaires.

Les URLs se trouvaient dans des fichiers JSON renseignés dans le code source des pages d'\ia. Un fichier JSON contient des informations représentées de manière structurée ; il s'agissait ici de métadonnées concernant le volume numérisé. Parmi celles-ci, une sous-section intitulée \texttt{data} contenait des métadonnées (longueur, largeur, etc.) et l'URI de chaque image. Ces images étaient au format JPEG et correspondaient à celles téléchargées sur le \sharedocs. 

Le problème posé par cette solution est les adresses n'étaient pas stables. En effet, au bout d'un certain temps, elles ne fonctionnaient plus car les informations avaient changé dans le fichier JSON source.

Aussi avons-nous exploré la piste du \iiif. L'\textit{International Image Interoperability Framework} permet d'afficher une image avec ses métadonnées dans le contexte d'une application web directement depuis le serveur où elle est stockée (ici, \ia). Les URLs de cette deuxième solution se trouvent dans les \og manifestes \iiif{} \fg{} des volumes : il s'agit des documents JSON contenant leurs métadonnées et référençant les points d'accès aux images (\cad{} leurs URIs dans le protocole \iiif).

Si ces images possèdent le même format que celles de la première solution, leur qualité est bien supérieure et permet d'effectuer des agrandissements d'une très grande profondeur. Du reste, le \iiif{} permet également de naviguer dans un volume en passant de page en page. Cette seconde solution est donc plus intéressante pour le projet \timeus{} dans la mesure où elle lui permet d'accéder d'une manière relativement simple à un document contenant un ensemble de données et de métadonnées qui pourront être valorisées au moment d'une édition en ligne.

Le script dont il est ici question, s'il se limite à insérer les URIs des images dans le code XML, commence par effectuer une requête pour lire le contenu des manifestes \iiif{} des \odm{}. Pour cela, il lit un CSV où nous avons enregistré les identifiants donnés par \ia{} aux numérisations, et les utilise pour compléter l'adresse des manifestes (\texttt{https://iiif.archivelab.org/iiif/<itemid>/manifest.json}). Ces lignes de code peuvent être réutilisées, probablement sous la forme d'une fonction, pour obtenir tout type de métadonnées issues des manifestes. En plus de rationaliser le stockage sur le \sharedocs{}, ce script prépare donc l'étape de la publication en ligne.

\section{Les données graphiques dans le flux textuel}

Un des principaux apport du \iiif{} pourrait être le traitement des objets graphiques qui, nous l'avons vu, sont nombreux dans les pages des \odm{} (\fig{} \ref{fig:ex_figures} p. \pageref{fig:ex_figures}). En particulier, les photographies, les figures ou les cartes pourraient bénéficier des fonctionnalités d'agrandissement afin de permettre à l'utilisateur de pleinement les prendre en considération.

Pour rendre cela possible, il faudrait disposer d'une évaluation du taux de détection des figures. Ce chiffre n'est pas connu, mais des relevés aléatoires laissent penser qu'il risque d'être inférieur à 60\%. Les tableaux sont les figures qui ont posé le plus de problème au script, nous y reviendrons.

Parmi les autres, plusieurs n'ont été que partiellement détectées. La carte de la page 439 de la monographie 90\footcite{mono090a} est ainsi parfaitement détectée et transposée dans un élément \texttt{<figure>} (\fig{} \ref{fig:odmfig90439}). À l'inverse, le plan d'une \og habitation cambodgienne à Pnom-Penh \fg{} est détecté en double --- peut-être en raison de la superposition d'une vue de face de l'habitation et de son plan --- et le titre de la figure est transcrit comme du texte (\fig{} \ref{fig:odmfig90453}). Dans la monographie suivante\footcite{mono090b}, la photographie pleine page \og Cambodgiens et amanites \fg{}, insérée entre les pages 484 et 485, a été retirée du texte et n'est pas présente dans les fichiers sous quelque forme que ce soit.

\begin{figure}
    \centering
    \begin{subfigure}{0.4\textwidth}
     \includegraphics[width=1\linewidth]{img/odm90_439.jpg}
     \caption{Page 439 : détection optimale de la carte.}
     \label{fig:odmfig90439}
    \end{subfigure}
    \hspace{5pt}
    \begin{subfigure}{0.4\textwidth}
     \includegraphics[width=1\linewidth]{img/odm90_453.jpg}
     \caption{Page 453 : détection en double et transcription du titre.}
     \label{fig:odmfig90453}
    \end{subfigure}
    \caption{Figures dans la monographie n° 90.}
    \label{fig:odmfig90}
\end{figure}

Ces erreurs de détection ne peuvent pas être résolues de manière automatique : aucune table des figures n'est présente dans les volumes. Un tel outil eut permis de cibler les pages à contrôler et de ne pas avoir à chercher visuellement les figures. Une intervention manuelle, possiblement longue, s'impose donc.

La présence de ces figures est également un élément à garder à l'esprit au moment de la reconstitution des paragraphes. En effet, certaines sont insérées en plein milieu de ces derniers. Pour ne pas perturber la reconstitution, il sera nécessaire d'ajouter une condition spécifiant que les balises \texttt{<p>} commençant pas une minuscule ne constituent pas une unité indépendante, et ce même si elles sont précédées d'une figure. Le problème de cette méthode est qu'elle présuppose que \lse{} a bien fait la différence entre une majuscule et une minuscule en début de phrase, ce dont on ne peut pas être totalement certain.

Ajoutons enfin que les figures de type carte, croquis ou photographie ont une importance relative du \pov{} de la donnée pure. Elles ne sont pas utiles aux chercheurs qui s'intéressent au vocabulaire du textile ou du monde ouvrier, ni à ceux qui souhaitent recueillir des données chiffrées. Dans le document XML, elles ne sont que des repères dont la nécessité ne se révélera qu'au moment de la transformation des fichiers pour une interface de visualisation.

\section{Les tableaux : images ou données ?}

À l'inverse, les tableaux qui jalonnent le corpus ont une importance considérable en raison des informations statistiques qu'ils contiennent. Plus particulièrement, les budgets des paragraphes 14 à 16 constituent \og la pièce centrale \fg{} des monographies leplaysiennes\footcite[p. 317]{savoyecontinuateurs}. Les \textit{Observations préliminaires} sont en effet entièrement tournées vers leur établissement ; le monographe n'arrête son enquête que lorsqu'il a établi un budget équilibré offrant une vue globale des recettes et des dépenses de la famille. Dans la démarche de Frédéric Le Play, \og l'argent est pris comme unité de mesure de la vie sociale, le budget étant la quantification, en termes de revenus et dépenses, de l'ensemble des activités de la famille \fg\footcite[p. 317]{savoyecontinuateurs}.

Cependant, d'un \pov{} technologique, il n'est pas simple de reproduire à l'identique ces tableaux. La TEI comporte un ensemble de balises destiné à cet effet --- \texttt{<table>}, \texttt{<row>} et \texttt{<cell>} --- mais le problème est ici fonction de la segmentation et de la compréhension par un script de la mise en page du tableau. Or cette mise en page peut se révéler sophistiquée. Ainsi, le texte est centré lorsqu'il s'agit de titre d'article ou de section, aligné à droite pour les postes de dépenses ou les recettes, aligné à gauche pour le total. Lorsque plusieurs postes de dépense successifs partagent une même formulation dans leurs intitulés, celle-ci n'est pas répétée mais signifiée par des traits de rappel.

Ces modulations permises par l'imprimé ne sont pas envisageables dans un tableau numérique, où toute donnée chiffrée doit correspondre à un libellé défini et non suggéré afin d'être rendu exploitable. Face à ce type d'information, un algorithme idéal devrait être capable de reproduire le processus cognitif de reconstruction effectué par le cerveau humain. En l'état actuel de l'avancée technique, faire en sorte que la machine différencie un trait de rappel d'une ligne de séparation entre deux cellules n'est pas aisé. Cela constitue presque un sujet de recherche à part entière et ne pas être mené dans le temps imparti par l'ANR au programme \timeus.

Le traitement des tableaux doit donc être pensé à l'aune de ces difficultés techniques et temporelles. Dans un essai de mise en scène des fichiers XML au format HTML, Alix Chagué a ainsi choisi de transformer les balises \texttt{<figure>} --- signifiant notamment la présence des tableaux --- en des icônes cliquables qui redirigent vers les images des pages où ces figures se trouvent. Le procédé est habile mais se fonde sur l'idée que toutes les figures ont été détectées, ce qui n'est pas encore le cas. Du reste, cela ne permet pas d'exploiter directement les informations des tableaux.

Pour autant, cette idée du tableau comme une image est sérieusement étudiée par le programme \timeus. L'effort d'ingénierie d'études se concentrerait alors sur la modélisation et l'implémentation d'une indexation fine des différents tableaux.

Il serait tout d'abord nécessaire d'effectuer un recensement exhaustif des tableaux des \odm. Ensuite, une étape de modélisation commencerait par l'analyse de leur structuration afin de faire ressortir les points communs et \textit{in fine} de constituer des catégories ; en parallèle, les chercheurs devront définir leurs besoins -- sont-ils intéressés par l'ensemble des informations, ou bien seules celles ayant trait au textile ? Une fois la typologie arrêtée et les besoins déterminés, il sera possible d'envisager l'implémentation d'une nouvelle couche d'encodage scientifique dans les fichiers XML. Celle-ci pourrait se traduire par deux actions.

D'une part, un identifiant \texttt{@xml:id} pourrait être donné à chaque titre de paragraphe, et ensuite référencé dans un attribut \texttt{@ana} (\texttt{analytic}). Peu intéressants pour les paragraphes de budget qui contiennent systématiquement des tableaux, ces attributs permettraient de signaler ceux que l'on peut rencontrer dans les paragraphes 6 à 10 (\textit{Propriétés}, \textit{Subventions}, \textit{Travaux et industries}, \textit{Aliments et repas}, \textit{Habitation, mobilier et vêtement}). D'autres part, un recensement des objets des tableaux --- dépenses de nourriture, dépenses pour l'habitation, dépenses pour le textile, etc. --- pourrait fournir une liste de valeurs pour un attribut \texttt{@type}. Au-delà de permettre un traitement rationnel des tableaux au regard des impératifs scientifiques et temporels, cette méthode pourrait servir de base à l'établissement de vues logiques ou de facettes de recherche conduisant à l'affichage des tableaux sur une base thématique.

La valorisation des objets graphiques n'est donc pas chose aisée. Elle peut se restreindre à leur simple reproduction doublée d'une indexation qui, si elle oriente le chercheur, ne le dispense pas d'effectuer lui-même l'extraction des données chiffrées ou textuelles contenues dans la figure. Il y a ici une limite au projet de numérisation du corpus.

\chapter{Données textuelles, données scientifiques}

\section{La qualité de l'\ocr}

L'extrême majorité des données des \odm{} est composée de texte. Ce texte peut être le support de différente études, qui se basent toutes sur la lecture. La lecture par l'oeil humain --- le corpus n'est pas réellement important et un chercheur peut en prendre intégralement connaissance ---, mais aussi la lecture par une machine, \cad{} l'analyse automatique. Ces deux opérations ne nécessitent pas un même niveau de qualité de l'\ocr{} et ne s'adressent pas au même public --- la première est accessible à un large panel, la seconde requiert l'assistance technique d'un ingénieur et l'exécution par une machine.

La qualité d'une \ocr{} peut être mesurée par différents indices, au niveau du caractère (\textit{character error rate}, CER) ou du mot (\textit{word error rate}, WER).  Le CER est obtenu grâce la formule suivante :

\[CER = \frac{S + D + I}{N}\]

où \textit{S} représente le nombre de substitutions (caractères dont la reconnaissance n'est pas correcte), \textit{D} le nombre de suppression (\textit{deletions}), et \textit{I} le nombre d'insertion, \cad{} les caractères qui ne sont pas présents dans la vérité terrain que l'on trouve pourtant dans l'\ocr. La somme de ces trois chiffres est divisée par le nombre total de caractères dans le fichier de vérité terrain (\textit{N}).

À partir d'une vérité terrain de 1300 lignes, Alix Chagué avait calculé un CER de 2,2\%\footcite[slide 16]{inria-pp}. Ce taux très faible est le signe d'une transcription de bonne qualité. Où se situent les erreurs restantes ?

Dans les \odm{} comme dans de nombreux autres corpus\footcite[p. 1]{sagot}, elles se concentrent sur les entités nommées. Au premier rang de ces dernières figurent les patronymes et les toponymes (\fig{} \ref{fig:mono-56-page-4-code}); les \og expressions temporelles et les expressions de quantité \fg{} ou encore \og les nombres, les formules chimiques, les unités monétaires \fg{} sont également concernés\footnote{Les entités nommées sont définies dans le domaine du traitement automatique du langage comme des \og unités faisant référence à une entité unique et concrète et réalisées par des noms propres (noms de personnes, d’organisations, d’artefacts ou de lieux) \fg{} : \cite[p. 4]{sagot}}. Ces entités sont en effet très peu représentées dans les fichiers de vérité terrain ; pour augmenter leur détection, un entraînement spécifique et focalisé sur elles serait nécessaire.

\section{Indexer les individus}

\begin{figure}[h]
    \centering
    \includegraphics[width=16cm]{img/index_tree.png}
    \caption{Arbre XML simplifié montrant l'organisation d'une entrée d'index.}
    \label{fig:index_tree}
\end{figure}

Cette défaillance dans la détection des entités nommées est un problème dans la mesure où les monographies de familles prennent pour sujet un groupe d'individus (les enquêtés) ; si leurs noms sont souvent anonymisés (\textit{M***}, \textit{F***}, etc.), les prénoms sont écrits en toute lettre.

Un projet de recherche mené par le CMH et le CHR concerne ces enquêtés et souhaite notamment les indexer en relevant les informations biographiques données par les monographes. L'objectif final est de lever l'anonymie d'une partie des individus en complétant voire en nuançant les informations du texte grâce à une exploitation des fonds d'archives départementaux ou municipaux.

Une tableau prosopographique au format CSV était déjà établi au moment de notre stage. Huit cent quarante-deux individus y été identifiés et décrits selon leur état civil et des critères sociaux. Les chercheurs nous ont demandé de procéder, dans un premier temps, à la transformation de ce tableau en un  index XML et, dans un seconde temps, de lier chaque entrée à l'individu correspondant dans les fichiers de monographie (\ann{} \ref{ann:feuille_route}, \issue{} 4). Du point de vue de l'ingénierie, cela se traduisait par la constitution automatique de l'index puis par l'implémentation dans les fichiers TEI des identifiants des individus cités dans le deuxième paragraphe.

Un fichier d'index au format XML repose sur un \texttt{<body>} où sont établies une ou plusieurs listes de personnes (\texttt{<listPerson>}), les individus y étant indexés au sein d'éléments \texttt{<person>}. Nous avons constitué une liste de personnes unique, car aucun besoin particulier n'a été exprimé quant à cette fonctionnalité du XML. Nous nous sommes concentrés sur les différentes catégories à faire figurer dans l'index et sur les balises pouvant les traduire (\fig{} \ref{fig:index_tree}).

L'identifiant des individus --- formé de celui de la monographie, de la lettre \textit{E} pour \textit{enquêté} et du numéro d'apparition --- constitue la valeur de l'attribut \texttt{xml:id} de la balise \texttt{<person>}. L'état civil se trouve ensuite dans un ensemble \texttt{<persName>} où le nom figure dans \texttt{<surname>} et le prénom dans \texttt{<forename>}. D'autre balises pourraient venir compléter cette section, notamment \texttt{<addName>} pour les surnoms ou les prénoms surnuméraires. Néanmoins, une reprise du tableau eut été nécessaire pour rendre l'usage de cet élément possible : dans l'état actuel, le surnom ou le second prénom se trouvent dans la même cellule que le prénom, ici entre parenthèses, là après un tiret et ailleurs à la suite du transitif \textit{dit}. Les informations de naissance (date et lieu) sont placées dans un ensemble \texttt{<birth>}, suivi de l'âge (\texttt{<age>}).

La situation matrimoniale est décrite, dans un ensemble \texttt{<event>} (évènement), par la balise \texttt{<desc>}. La TEI conseille de décrire l'évènement de manière normalisée par un \texttt{<label>} placé avant \texttt{<desc>}, mais, dans la mesure cet index ne compte qu'un seul évènement, nous avons choisi de simplifier la structure et d'en préciser la nature grâce à un attribut \texttt{@type} au niveau de \texttt{<event>}. Suivent deux indicateurs contenant le positionnement de l'individu dans la cellule familiale (\texttt{<state>} : chef de famille, femme, fille) et son activité, qu'il s'agisse d'un apprentissage, d'un métier ou d'un travail à la tâche ou à la journée (\texttt{<occupation>}). Dans une \texttt{<note>} finale, nous avons fait figurer la date de référence pour les calcules des dates de naissance et de mariage et le rappel du titre de la monographie où apparaît l'individu.

\section{Corriger les transcriptions ?}

\begin{figure}
    \centering
    \begin{subfigure}{0.8\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/mono-56-page-4.jpg}
    \caption{}
    \label{fig:mono-56-page-4}
    \end{subfigure}
    
    
    
    \begin{subfigure}{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/mono-56-page-4-code.png}
    \caption{}
    \label{fig:mono-56-page-4-code}
    \end{subfigure}
    \caption[Lliste d'individus et de son encodage (\no{} 56)]{Liste d'individus et son encodage (série 2, volume 2, monographie \no{} 56, page 4.}
    \label{fig:ex-mono-56}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=14cm]{img/mono-23-page-209.jpg}
    \caption[Liste d'individus et de son encodage (\no{} 23)]{Liste d'individus et son encodage (série 1, volume 3, monographie \no{} 23, page 209.}
    \label{fig:mono-23-page-209}
\end{figure}

Dans chaque volume, le paragraphe \og État civil de la famille \fg{} commence par une liste standardisée des membres de la famille. La figure \ref{fig:ex-mono-56} montre une comparaison entre le texte d'origine de la monographie 56, sa transcription et son encodage par \lse\footcite[p. 4]{mono056a}. Les erreurs concernant la reconnaissance des entités nommées y apparaissent clairement : sur quatre prénoms, tous sont transcrits avec une distance d'édition importante et le nom de famille \og B*** \fg{} n'est absolument pas reconnu.

Ici, ces erreurs ne posent pas qu'un problème de compréhension : elles empêchent la réalisation de la deuxième mission qui nous avait été confiée et qui consistait à lier les individus à leurs identifiants. En effet, cette opération se serait normalement traduite par l'implémentation d'une balise \texttt{<persName>} autour de chaque ensemble prénom et nom, avec un attribut \texttt{@ref} ayant pour valeur l'identifiant donné dans le fichier d'index. L'implémentation automatique aurait pu s'effectuer de plusieurs manières, et notamment par la recherche du prénom : cela n'est pas possible du fait des distances d'édition trop grande.

Une autre solution aurait procédé de l'observation que, les paragraphes deuxièmes commençant toujours par une phrase de type \og La famille comprend X personnes \fg{}, \og La famille comprend \fg{} ou \og Les membres de la famille sont \fg{}, les lignes qui suivent correspondent aux membres de la famille. Il eut été dès lors possible, grâce à l'ordre d'apparition noté dans le CSV, de considérer que la première de ces lignes est celle du premier individu, et ainsi de suite. Ce procédé se base cependant sur l'idée que les items des listes se trouvent sur une seule ligne, ce qui n'est pas toujours le cas (\fig{} \ref{fig:mono-23-page-209}). Du reste, placer une balise \texttt{<persName>} sur l'ensemble d'une ligne serait revenu à corrompre son usage, normalement réservé à \og un nom propre ou une expression nominale se référant à une personne \fg{}\footnote{\textit{TEI element persName (personal name)}, \textit{TEI Guidelines} : \url{https://tei-c.org/release/doc/tei-p5-doc/fr/html/ref-persName.html} (consulté le \today).}.

Ainsi, dans tous les cas, la réalité des transcriptions empêchait l'implémentation automatique de la balise et nous confinait à une opération de correction.  Or celle-ci ne peut pas être menée sans une intervention humaine, qu'elle soit entièrement manuelle ou semi-automatisée grâce à un logiciel de suggestion de correction\footcite[p. 1]{sagot}. Ces logiciels --- par exemple, \textit{Antidote}\footnote{Présentation : \url{https://www.antidote.info/fr/} (consulté le \today).} --- s'appuient sur un lexique et, \og pour chaque mot inconnu, [cherchent] des candidats proches (par exemple en termes de distance d’édition) qui figurent dans le lexique et choisissent en prenant en compte la fréquence des candidats, le contexte, ou éventuellement un poids associé au type d’erreur présumé \fg{}\footcite[p. 1-2]{sagot}. Une telle intervention, quels que soient les moyens choisis pour sa réalisation, serait du reste possiblement très longue et dans tous les cas coûteuse\footcite[p. 14]{en-tal}.

La librairie \texttt{pyspellchecker} de Python peut également constituer une solution. Elle utilise un dictionnaire et la distance de Levenshtein pour identifier des cacographies et proposer, selon les paramètres, soit une correction soit plusieurs candidats. Par défaut, la distance est de 2, elle peut être ramenée à 1, ce qui signifie que selon le réglage la librairie va considérer qu'il y a eu une ou deux opérations de suppression, de substitution ou d'insertion.

Un test sur les cinq premiers \texttt{<p>} de la figure \ref{fig:mono-56-page-4-code} a donné de bons résultats (tabl. \ref{tabl:pyspellchecker}). Le script en question cherche le texte des balises \texttt{<p>}, le nettoie en enlevant les signes de ponctuation, et crée des listes où chaque mot correspond à un item. \texttt{Pyspellchecker} analyse ensuite ces mots et peut, selon les paramètres, proposer une solution unique grâce à la méthode \texttt{.correction{}} ou un panel de candidats grâce à la méthode \texttt{.candidates()}.

\begin{table}
    \begin{tabular}{|c|c|c|}
\hline
    Vérité terrain & OCR & Correction \\ \hline
    \textsc{Louis} & LOUIS & --- \\ \hline
    \textsc{Joseph} & JosEP & joseph \\ \hline
    \textsc{Félicité} & FLICITÉ & félicité \\ \hline
    \textsc{Josèphe} & JosÉPH & joseph \\ \hline
    \textsc{Gaspardine} & GAPARDINE & gagarine \\ \hline
    \textsc{Marguerite} & MARGUERIT & marguerite \\ \hline
    \textsc{Louise} & ILoIE & loin \\ \hline
    \textsc{Félicité} & FÉLCITÉ & félicité \\ \hline
    \end{tabular}
\caption{\label{tabl:pyspellchecker} : Comparaison entre la vérité terrain, la transcription effectuée par l'\ocr{} et la proposition de correction de \texttt{pyspellchecker}.}
\end{table}

Le tableau \ref{tabl:pyspellchecker} montre que, sur sept transcriptions fausses, \texttt{pyspellchecker} permet d'en corriger quatre de manière exacte (\textit{Félicité} à deux reprises, \textit{Joseph} et \textit{Marguerite}). La vérité terrain est presque approchée pour \textit{Josèphe} : le \textit{e} final et l'accent grave manquent. Ce test pourrait ainsi être transformé en un script d'aide à la correction, augmenté d'une fonctionnalité de remplacement après qu'un opérateur ait validé la proposition de \texttt{pyspellchecker}.

Une autre librairie Python, \texttt{pygrammalecte}, peut être utilisée pour générer un rapport d'identification des erreurs. La différence avec la librairie précédente est qu'elle fonctionne avec un dictionnaire et est capable, en plus de détecter les erreurs, de les répartir dans une typologie (orthographe, grammaire, ponctuation). Cette librairie est utilisée par le service humanités numériques de l'École nationale des chartes pour évaluer la qualité des \ocr{} des positions des thèses pour le diplôme d'archiviste paléographe, et permet de générer un rapport d'erreur sous la forme d'un fichier JSON\footnote{Voir notamment la fonction \texttt{ocrquality} dans le script \texttt{encpos\_control.py}, ligne 72 : \url{https://github.com/chartes/encpos/blob/38ad0277a03467642dd8a329a37237c9e663f4d1/utils/encpos_control.py\#L72} (consulté le \today).}.

On le voit, il est possible de développer des solutions en interne pour corriger une \ocr. Si elles n'automatisent pas totalement cette correction, elles permettent de réduire son coût budgétaire en épargnant le recours à un prestataire externe ou l'achat d'un logiciel dédié.

\chapter{Une valorisation plurielle}

\section{Édition papier, édition numérique}

Un postulat commun veut qu'une édition numérique offre à son utilisateur plus de possibilités qu'une édition papier n'en offre à son lecteur\footnote{\og \textit{Often these descriptions glance at their print predecessors, usually with expressions of how much more these digital editions can contain than ever could be included in print editions, and how much more the reader can do with them} \fg{} : \cite[p. 105-106]{robinson}.}. Le numérique permet en effet de concevoir des plate-formes sur mesure pour la consultation des textes, et les outils des humanités numériques permettent à \og de nombreuses données non interrogeables jusqu’à présent [d'être] l’objet d’enquêtes \fg\footcite[p. 20]{duval}. \og Des structures cachées, des faits de système difficilement décelables à l’œil et à la main \fg{} deviennent ainsi accessibles\footcite{duval}.

Dans le programme \timeus{}, l'édition numérique des \odm{} a éveillé l'intérêt d'au moins trois participants. Le LARHRA de l'université de Lyon 2, dans le strict respect des objectifs du programme, souhaite utiliser les informations économiques fournies par les tableaux de budget et celles d'ordre prosopographique contenues dans le paragraphe \og §2 --- État civil de la famille \fg. L'équipe ALMAnaCH d'Inria a pour sa part la volonté de puiser dans les champs lexicaux des mondes ouvrier et industriel afin d'alimenter des algorithmes de traitement automatique du langage (TAL). Enfin, le Centre de recherches historiques (CRH) veut fournir à la communauté scientifique une édition numérique des \odm{} qui intégrerait des informations matérielles sur la constitution du corpus et le travail de la \sess.

On le voit, les objectifs poursuivis par ces entités sont très divers. Les matériaux qui permettront de les réaliser le sont tout autant : le LARHRA et ALMAnaCH ont besoin de données brutes issues des tableaux statistiques (des chiffres) et du texte (des mots), tandis que le CRH agrège des métadonnées inédites qui proviennent de plusieurs corpus. Le LARHRA a également besoin d'une transcription de qualité pour s'assurer de la viabilité des informations prosopographiques du second paragraphe.

Cette pluralité de directions illustre la tension qui traverse les éditions numériques : elles portent avant tout sur un document et non sur une \oe{}uvre\footnote{\og \textit{Two decades of making digital editions, and recent papers about digital editions, have moved the needle away from the “work” to the “document”, to the point where we might need only think of “documents”} \fg{} : \cite[p. 107]{robinson}}. Cette distinction est issue de la triade document, texte, \oe{}uvre (\textit{document}, \textit{text}, \textit{work}) qui désigne les dimensions matérielle, linguistique et intellectuelle d'un écrit\footnote{\og \textit{Work} désigne le texte de l’auteur, éventuellement le texte correspondant à la volonté de l’auteur, et implique la notion d’authenticité ; \textit{text} dénomme la séquence linguistique attestée dans un document transmettant l’œuvre ; enfin \textit{document} est une manifestation physique d’un text \fg{} : \cite[p. 15-16]{duval}.}. De fait, l'encodage que nous avons décrit dans la partie précédente fait la part belle au document \odm{} de l'Université de Toronto, digitalisé par \ia. \transkribus{} et le script \lse{} mobilisent la TEI pour mener à bien la reproduction fidèle du document grâce à des ensembles \texttt{<facsimile>}\footcite[p. 124]{robinson}. L'\oe{}uvre n'est pas pour autant oubliée. Elle se rencontre dans la structure logique leplaysienne, là encore reproduite fidèlement par le biais des divisions et des titres.

Cette coexistence apparente n'a cependant pas vocation à durer : dans la vision de \timeus, c'est bien l'\oe{}uvre et non le document qui doit prendre le dessus. Il n'est pas question de concevoir un support de consultation qui présenterait des échantillons successifs correspondant au contenu d'une page. Cela reviendrait à reproduire l'interface de visualisation d'\ia, tout en donnant une réalité concrète à l'hypertexte qui est \og en gestation dans les tables, index et diverses aides à la lecture de consultation déjà présentes \fg{} dans les volumes\footcite[p. 19]{duval}.

Le premier essai d'édition des fichiers XML, mené par Alix Chagué, consiste ainsi en un document HTML où l'intégralité du texte du premier volume est reproduit, organisé en fonction de la structure logique et non des zones de segmentation\footnote{Cette démonstration est visible à cette adresse : \url{http://demo-leplay.herokuapp.com/volume_parsed_test.html} (consulté le \today).}. Elle pourrait cependant être améliorée avec des informations issues du document, à commencer par la traduction, par exemple entre crochets droits, des balises \texttt{<pb>} à chaque changement de page.

Ces observations montrent qu'une édition numérique se doit d'être équilibrée et de rendre compte à la fois du document-texte et de l'\oe{}uvre-texte\footnote{\og \textit{A scholarly edition must, so far as it can, illuminate both aspects of the text, both text-as-work and text-as-document} \fg{} : \cite[p. 123]{robinson}.}, sans quoi elle risque de perdre tant ses lecteurs\footnote{\og \textit{But there are dangers here. (...) Facsimile editions in print form are of very little use to the reader, or even to scholars, whose interest (...) is likely to be in questions of how the received text changed over time, how it was received, how it was altered, transformed, passed into different currencies. If we make only digital documentary editions, we will distance ourselves and our editions from the readers} \fg{} : \cite[p. 127]{robinson}.} que ses utilisateurs\footnote{\og La lisibilité des éditions électroniques n’a rien à envier à celle des éditions papier. (...) Parfois, les interfaces ne sont pas intuitives et requièrent une longue familiarisation ; d’autres fois, des aides à la lecture systématiquement présentes dans les éditions papier disparaissent \fg{} : \cite[p. 21]{duval}.}.

\section{Retrouver les fascicules dans le volume}

Au premier abord, il est aisé de considérer les \odm{} comme une \oe{}uvre. Mais plus on progresse dans l'histoire de cette entreprise, plus cette idée première se fragilise. En effet, si ce corpus s'incarne aujourd'hui dans des volumes, ceux-ci étaient autrefois des fascicules. Il est en outre constitué de monographies réparties en trois séries, tout en formant un ensemble cohérent dont les membres \og prennent sens les uns par rapport aux autres \fg{}\footcite[p. 5]{chenu}. D'une certaine manière, la volonté du CRH d'explorer l'histoire matérielle du corpus pour compléter les métadonnées des fichiers XML remet en question l'existence de \og l'\oe{}uvre \odm{} \fg{}.

Les enjeux de l'édition numérique d'un texte imprimé aux \textsc{xix}\ieme{} et \textsc{xx}\ieme~siècles divergent de ceux sous-entendus par l'édition de texte imprimé lors de siècles antérieurs. Aucune ré-impression n'est ici attestée : le texte est le même d'un exemplaire à l'autre, et ce jusque dans ses imperfections. C'est la raison pour laquelle l'encodage ne fait aucun effort de lématisation et qu'il s'appuie, de fait, sur les seules numérisations des exemplaires de Toronto.

Dans l'encodage d'un texte imprimé tel que \lodm{}, le défi se situe non pas au niveau du texte et de ses différentes versions, mais bien dans la restitution de la génétique matérielle qui a amené à la constitution des volumes. Comment traduire dans l'encodage les stratégies mises en place par les différents relieurs pour fondre les fascicules dans le volume ? Dans les exemplaires de la Bibliothèque nationale de France, les feuillets liminaires des fascicules ont été placés après les tables, là où ailleurs ils ont été conservés dans le flux du texte. Il y a ici une subtilité dont l'encodage de niveau \og document \fg{} ne se préoccupe pas, et qui portant est essentiel pour les deux autres niveaux.

Rappelons qu'un document TEI est divisé en deux grandes parties, que sont le \texttt{<teiHeader>} dévolu aux métadonnées et le \texttt{<text>} contenant le document. C'est dans le premier que les observations relevées par le CRH doivent prendre place, et plus précisément dans la section \texttt{<sourceDesc>}. Plusieurs ensembles de description des exemplaires (\texttt{<msDesc>}) peuvent en effet y être mobilisés afin de rendre compte des différences entre les exemplaires parisiens et ceux de Toronto.

Ces apports se bornent donc aux métadonnées, l'encodage produit par \lse{} et amélioré par notre reprise n'en sera pas affecté. Celui-ci n'est pas pour autant fixé. La place des objets graphiques reste à définir --- comment traiter les tableaux de manière à ce que le LARHRA puisse exploiter les chiffres qui s'y trouvent ? Si la structure logique est en place, le système des renvois entre les monographies n'a pas été valorisé ; les informations prosopographiques doivent également être repérées et signalées.

\section{\textit{Quid} de la donnée ?}

Une question demeure au sujet des fichiers des \odm{} : quel avenir pour eux, non pas dans une mise en scène quelconque, mais en tant que données brutes ? Le choix de recourir au format XML-TEI montre que \timeus{} entend assurer la conservation de ces fichiers. La TEI est en effet un standard de données maintenu par une communauté active. En rédigeant une cartographie du corpus et surtout une ODD, nous avons documenté la pratique éditoriale et favorisé sa compréhension par d'autres chercheurs ou projets qui souhaiteraient réutiliser l'encodage. Le dépôt en ligne sur le \gitlab{} de l'Inria assure la conservation de l'historique de nos interventions.

Néanmoins, le dépôt \gitlab{} est aujourd'hui en accès restreint, aussi le corpus n'est-il pas en libre accès (\openaccess). Cette restriction est bien évidemment due au fait que le travail n'est pas achevé et sera levée à la fin du programme ANR. Pour autant, \timeus{} souhaite démultiplier les espaces de conservation en clonant le dépôt \gitlab{} sur \github. Ces deux sites offrent des services d'hébergement utilisant la technologie git, à la différence que \github{} est une entreprise commerciale possédée par Microsoft et ne dépendant donc pas d'une institution publique dont les orientations budgétaires peuvent changer. De fait, des procédure de migration existent entre les deux plate-forme et concernent non seulement les \commits{}, mais aussi les \issues{} et les \mergerequests.

Une autre piste étudiée est de déposer les textes des \odm{} sur \wikisource. Il s'agit d'une bibliothèque numérique maintenue par la fondation Wikimédia, gratuite, en \openaccess, contenant des \opendata{} (aucune restriction sur l'usage des données) et ouverte à tous ceux qui souhaitent y contribuer. Le problème est que les fichiers XML ne peuvent pas être versés en l'état et qu'ils doivent être modifiés voire transformés dans la syntaxe de la Wikimedia, le wikicode. Cette opération peut donc s'avérer coûteuse et le transfert de la totalité des informations n'est pas garanti ; les dépôts git sont de ce \pov{} de bien meilleures solutions.

Les données des \odm, déjà standardisées, documentées et pérennisées, bientôt en \textit{open access}, sont ainsi assurées d'être indépendantes de toutes les mises en scène auxquelles elles pourraient se prêter\footcite[p. 63]{jolivet}.

Ces dernières années, l'écosystème de la donnée s'est néanmoins tourné vers le \linkeddata, \cad{} la diffusion des données \og de manière structurée, de manière à favoriser leur mise en relation \fg\footcite[p. 66]{jolivet}. Une des conditions de cette mise en relation est l'interopérabilité des données. Les fichiers des \odm{} satisfont-ils à cette demande ? En d'autres termes, la question est de déterminer si les données qu'ils contiennent peuvent être enrichies par des informations venues d'autres formats, ou bien si elles peuvent être réutilisées par des programmes de recherche ou avec d'autres corpus dont le standard n'est peut-être pas la TEI, ou qui n'ont pas le même usage de la TEI. C'est là où le bât blesse : la TEI, du fait de son extrême souplesse, permet des pratiques éditoriales qui peuvent être très éloignées. Ce que les corpus gagnent en précision et en fidélité par rapport au(x) document(s) d'origine, ils le perdent en interopérabilité\footcite[p. 61-62]{jolivet}.

Une manière de rendre les données des \odm{} interopérables seraient de les rendre accessibles \textit{via} une API. Une API web est une interface qui offre un accès direct et simplifie l'interaction avec la donnée structurée par les ingénieurs et les programmeurs et non transformée pour les utilisateurs. Ces données, souvent codées en JSON, peuvent aussi être servies en XML.